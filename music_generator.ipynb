{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OvtYbPtjApy0",
    "outputId": "d07f7f23-374a-40e3-c423-2fcfbe8245e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzK7NTBxApy8"
   },
   "source": [
    "# Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "RrmNz0W4Apy9",
    "outputId": "2d126990-03f8-4e4e-e379-88f144ee0882"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8071: expected 8 fields, saw 11\\nSkipping line 13038: expected 8 fields, saw 9\\nSkipping line 19137: expected 8 fields, saw 10\\nSkipping line 19565: expected 8 fields, saw 14\\nSkipping line 31405: expected 8 fields, saw 9\\nSkipping line 34976: expected 8 fields, saw 10\\nSkipping line 40651: expected 8 fields, saw 13\\nSkipping line 42611: expected 8 fields, saw 11\\nSkipping line 43461: expected 8 fields, saw 11\\nSkipping line 43502: expected 8 fields, saw 12\\nSkipping line 49435: expected 8 fields, saw 11\\nSkipping line 49444: expected 8 fields, saw 12\\nSkipping line 55682: expected 8 fields, saw 9\\nSkipping line 57135: expected 8 fields, saw 9\\nSkipping line 58018: expected 8 fields, saw 11\\nSkipping line 61264: expected 8 fields, saw 19\\nSkipping line 62817: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 68567: expected 8 fields, saw 9\\nSkipping line 69526: expected 8 fields, saw 10\\nSkipping line 70174: expected 8 fields, saw 9\\nSkipping line 72601: expected 8 fields, saw 12\\nSkipping line 74311: expected 8 fields, saw 11\\nSkipping line 82671: expected 8 fields, saw 9\\nSkipping line 84946: expected 8 fields, saw 34\\nSkipping line 85984: expected 8 fields, saw 10\\nSkipping line 88010: expected 8 fields, saw 10\\nSkipping line 105174: expected 8 fields, saw 10\\nSkipping line 108808: expected 8 fields, saw 9\\nSkipping line 110212: expected 8 fields, saw 13\\nSkipping line 115051: expected 8 fields, saw 9\\nSkipping line 119445: expected 8 fields, saw 9\\nSkipping line 121202: expected 8 fields, saw 9\\nSkipping line 121529: expected 8 fields, saw 9\\nSkipping line 130307: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 145767: expected 8 fields, saw 19\\nSkipping line 155717: expected 8 fields, saw 10\\nSkipping line 157863: expected 8 fields, saw 10\\nSkipping line 174547: expected 8 fields, saw 21\\nSkipping line 175168: expected 8 fields, saw 9\\nSkipping line 176887: expected 8 fields, saw 23\\nSkipping line 178353: expected 8 fields, saw 24\\nSkipping line 182968: expected 8 fields, saw 13\\nSkipping line 194184: expected 8 fields, saw 12\\nSkipping line 195545: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 207647: expected 8 fields, saw 9\\nSkipping line 209227: expected 8 fields, saw 9\\nSkipping line 216890: expected 8 fields, saw 11\\nSkipping line 217818: expected 8 fields, saw 10\\nSkipping line 222429: expected 8 fields, saw 13\\nSkipping line 222796: expected 8 fields, saw 14\\nSkipping line 224539: expected 8 fields, saw 13\\nSkipping line 225488: expected 8 fields, saw 12\\n'\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"transaction4.csv\"\n",
    "data = pd.read_csv(data_dir, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y12JUY0iApzL"
   },
   "outputs": [],
   "source": [
    "text = data['tran_text'].astype('str')[:500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZWlk4HfApzO"
   },
   "outputs": [],
   "source": [
    "all_text = \". \".join (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "22asDWxnApzS",
    "outputId": "be5c276c-1c19-4546-b484-9b77c62fecf9",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Big bear. More. Me plus other haunted house. Alan. So bad u don. Bailar. :smiling_face_with_horns:. Wifi for last year. Thai market. Spotify. Stock in trade. food and more. :B_button_(blood_type):️. Lyft. dinnah. soo shee. :automobile::dashing_away::dashing_away:. Friendship. November parking. Ball's. I played myself. Pool. thurs.. :OK_hand:. i fucked up. Cleaning ladies. g. Grub. Nu friends hat. BK & AL. Food. Ravioli ravioli give me the formioli. Drunchies. Mcdd. shake shack bk. Ty man. Thank you!!. Bowls. Sitting on my furniture.. Cutting the grass. @Tynan-Kelly. Drank. More gas. Bun. Raaaaage. Taxi :sunrise:. Cheepotle. deepher dude shirt :). I’m stupid. Food+. :steaming_bowl:. Anal beads and other sexual accessories. SC(NH2)2. FOOOD. Post-fight pizza. jiro’s :sushi:. Alex's champagne. Jimmy's 2. :taxi:. Bubble tea!. Cover. Breaking glass. :money-mouth_face::money-mouth_face::money-mouth_face:. Get yourself a coffee and know that you and your fam are in my thoughts :red_heart:️. WiFi - dec. $$$. Masks. Thai 55. Babysitting. Slush. Honey, I forgot the honey. I want to retire. Met in the middle. Yeah. :bowling:. :taxi:. Groceries. Uber!. Subway. Ubers. Another shirt for another human. Uc broccoli. June gas bill. My respects. Donation. Boxes. Piza. スレ-ヤス is chill. Christian :french_fries:. December gas. Bet. :crab:. good fucks. :bento_box::bento_box::pot_of_food:. :woman_dancing_light_skin_tone:. Hoodie -Melody Eng. DIVING into summer :sweat_droplets:. Last night. Board. Being a hero. Snackss. :cooked_rice::cooked_rice::cooked_rice:. vball :volleyball:. Port. spagooter. Avocados. “Like”. vroom vroom. I’m an agent of chaos. thx bish. :taxi: minus Uber chudap. ub to ema. hereditary & lyft. Lmao. Internet. In. 皮蛋肉粥. Team grandmas \\U0001f9f6. :pig_nose:. <3 ? <3. Bulrusher. Blaze it. hat. Choc shake from the other night. :United_Kingdom:. This girl Un invited from next poker night. Benny fierro. Diwali tix :sparkles:. :pizza:. food. :cookie:. SAUCE. Four fifths. :dollar_banknote:. Jk we only want 5. Yum yum. :clinking_glasses:. 123. Christy!. Rip. Brockhampton. :pizza:. :panda_face:. Stickerrzzz. Boba. 1/2. Free :red_heart:. Food. nan. Plant. G. mcd. :blue_circle::blue_circle::face_blowing_a_kiss:. Blunt and 6 for you know what. I insist. Potle. Narp. Yuhyuhyuhyuhyuhyuhyuhyuhyuhyuhyuhyuhyujyuhyuhyuhyujyuhtuhyuhyuhyuhyyhtuhy. minus dinner. Brunch. :popcorn::popcorn:. Oops. #kool #kar. Uber. SOAP. H. comcast. Size small!!. :dollar_banknote::BACK_arrow:. :cooking::hot_beverage:. Food. Fooood. :pizza::peach::eggplant:. Brunei Hotel. feast table. Nekter. Grapes. Freshly meals. :baby_bottle:. Shahag. :person_getting_haircut_light_skin_tone:. :tropical_drink:. NYE (includes the fees). ComEd. :smiling_cat_face_with_heart-eyes::smiling_cat_face_with_heart-eyes::smiling_cat_face_with_heart-eyes::smiling_cat_face_with_heart-eyes::smiling_cat_face_with_heart-eyes::smiling_cat_face_with_heart-eyes::smiling_cat_face_with_heart-eyes:. :house::money_with_wings:. Cuz I'm a man if my word. :watch:. Ubs. :red_heart:. Gift. Zipcar. Cooks. :thumbs_up::thumbs_up::hundred_points:. Wiksey. Feeding me :sweat_droplets:. Fud. Zebala. Uber everywhere. :tangerine::sleeping_face:. Rafting picz. The racquet, bitch. Ralph Run. Olive :). Oober. My b. Bag. Füd. :ticket:. Different world man. Successful cards. Cap back. 별밤. I finally completed this. +50 cents for telling me who can help with vlookup. :thumbs_up_medium_skin_tone:. cash only :dollar_banknote:. ラーメン (°▽°) ʕ•̫͡•ʕ•̫͡•ʔ•̫͡•ʔ•̫͡•ʕ•̫͡•ʔ•̫͡•ʕ•̫͡•ʕ•̫͡•ʔ•̫͡•ʔ•̫͡•ʕ•̫͡•ʔ•̫͡•ʔ. :woman’s_clothes:. cookies. ha ro ld s. Cheese with the boys. :deciduous_tree::deciduous_tree::deciduous_tree::handshake::raising_hands:. for mailing. Ticket 2/2. Workshop. alc handle. Your love pt 1. More tickets. Hh. Uber downtown to :movie_camera:. Prty. far past medium ugly. FIFA let’s goooo. Navy polo + soap dish. ayce sushi. :face_blowing_a_kiss::face_blowing_a_kiss:. taco. :racing_car:. .. Payout. the middle one. Louvre  + :soft_ice_cream:. 2nd course. Beauty is truth something something. dumplings. Mcd. :eggplant:. :tongue:. Yummy. :doughnut:. Uber. Uba. :oncoming_taxi:. groceries + rug. cha. Axe. piazza. Stolen food. Tickets. Espressos. :white_medium_star:️. 1 coconut bun 1 red bean bun!. spicy füd. h night. Lobsters don't pay themselves. NO. Daru. L o l. Tea. Big vik. FIFA 16 pt1. Winter peas. I'm speechless. Hingkee. :tumbler_glass:. :cookie:. :money_bag:. U ski bro. food. Boba. Netflix. Hot dog water :squinting_face_with_tongue::smiling_face_with_sunglasses::smiling_face_with_sunglasses::smiling_face_with_sunglasses:. :shallow_pan_of_food:. Tide pods + bounce sheets. Tickets plz. :cooking::tropical_drink:. :fork_and_knife_with_plate:. Dues and tourney. Uber. Helping a broke bitch out. dd. cuatro crazy. uber. stake. Sauce. happy birthday. :oncoming_police_car:. Y. Some rungood. vibe. Last night. Home supplies. :beaming_face_with_smiling_eyes:. For bills on splitwise.com. Big Mac meal. TEA SIS. :crown:. for froyo. Ha\""
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-jGo9AVApzX"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6H93h9LApzY"
   },
   "outputs": [],
   "source": [
    "def vectorizing_seq (text, maxlen, step):    \n",
    "    \"\"\"\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param step: sample a new sequence every n steps\n",
    "    :type  step: int\n",
    "    :returns: (Numpy boolean array of shape \n",
    "                    (Number of sequences, maxlen, number of distinct character),\n",
    "               Numpy boolean array of shape \n",
    "                    (Number of sequences, number of distinct character),\n",
    "               dictionary mapping a character to its integer placeholder)\n",
    "    :rtype:   (numpy.ndarray, \n",
    "               numpy.ndarray, \n",
    "               dict)     \n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = [] # hold extracted sequences\n",
    "    next_chars = [] # hold next characters for each corresponding sentence\n",
    "\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "\n",
    "    print('Number of sequences:', len(sentences))\n",
    "\n",
    "    chars = sorted(list(set(text)))\n",
    "    print('Unique characters:', len(chars))\n",
    "    char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "    print('Vectorization...')\n",
    "\n",
    "    # one hot encoding the characters into binary arrays\n",
    "    x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "        \n",
    "    return x, y, char_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VAiUksj_Apza"
   },
   "outputs": [],
   "source": [
    "def create_model(x, y, maxlen, epochs, chars):\n",
    "    \"\"\"\n",
    "    Creates and trains a model.\n",
    "    :param x: Numpy boolean array of shape \n",
    "                    (Number of sequences, maxlen, number of distinct character)\n",
    "    :type  x: numpy.ndarray\n",
    "    :param y: Numpy boolean array of shape \n",
    "                    (Number of sequences, number of distinct character)\n",
    "    :type  y: numpy.ndarray\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param epochs: number of training iterations\n",
    "    :type  epochs: int\n",
    "    :param chars: list of unique characters\n",
    "    :type  chars: list\n",
    "    :returns: trained keras model\n",
    "    :rtype:   keras.engine.sequential.Sequential\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.GRU(\n",
    "        32,\n",
    "        return_sequences=True,\n",
    "        input_shape=(maxlen, len(chars)))\n",
    "    )\n",
    "    model.add(layers.GRU(\n",
    "        64,\n",
    "        input_shape=(maxlen, len(chars)))\n",
    "    )\n",
    "    model.add(layers.Dense(\n",
    "        len(chars), \n",
    "        activation='softmax')\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    optimizer = optimizers.RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    model.fit(x, y, batch_size=128, epochs=epochs)\n",
    "\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "keseYGPyApzd"
   },
   "outputs": [],
   "source": [
    "def train_model_from_text(text, maxlen=6, step=12, epochs=10):\n",
    "    \"\"\"\n",
    "    Given text, train the model.\n",
    "    \n",
    "    :param text: A string with all the text together.\n",
    "    :type  text: str\n",
    "    :param maxlen: the length of a sequence to extract as train\n",
    "    :type  maxlen: int\n",
    "    :param step: sample a new sequence every n steps\n",
    "    :type  step: int\n",
    "    :param epochs: number of training iterations\n",
    "    :type  epochs: int\n",
    "    :returns: (trained keras model,\n",
    "               dictionary mapping characters to digit representations)\n",
    "    :rtype:   (keras.engine.sequential.Sequential,\n",
    "               dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y, char_indices = vectorizing_seq(text, maxlen, step)\n",
    "    chars = list (char_indices.keys())\n",
    "    model = create_model(x, y, maxlen, epochs, chars)\n",
    "    \n",
    "    return model, char_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZbKJMKrApzg"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Compute new probability distribution based on the temperature\n",
    "    Higher temperature creates more randomness.\n",
    "    \n",
    "    :param preds: numpy array of shape (unique chars,), and elements sum to 1\n",
    "    :type  preds: numpy.ndarray\n",
    "    :param temperature: characterizes the entropy of probability distribution\n",
    "    :type  temperature: float\n",
    "    :returns: a number 0 to the length of preds - 1\n",
    "    :rtype:   int\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mClhlohUApzj"
   },
   "outputs": [],
   "source": [
    "def text_generate(model, text, char_indices, maxlen=4, temperature=1.0, textlen=10):\n",
    "    \"\"\"\n",
    "    Generate text based on a model.\n",
    "    \n",
    "    :param model: trained keras model\n",
    "    :type  model: keras.engine.sequential.Sequential\n",
    "    :param text: lyrics\n",
    "    :type  text: str\n",
    "    :param char_indices: dictionary mapping a character to its integer placeholder\n",
    "    :type  char_indices: dict\n",
    "    :param maxlen: maximum length of the sequences\n",
    "    :type  maxlen: int\n",
    "    :param textlen: Number of characters of generated sequence\n",
    "    :type  textlen: int\n",
    "    \"\"\"\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1) \n",
    "    generated_text = text[start_index: start_index + maxlen] \n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "    \n",
    "    chars = list (char_indices.keys())\n",
    "    \n",
    "    print('------ temperature:', temperature)\n",
    "    sys.stdout.write(generated_text)\n",
    "    for i in range(textlen):\n",
    "        sampled = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(generated_text):\n",
    "            sampled[0, t, char_indices[char]] = 1\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = chars[next_index]\n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]\n",
    "        sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gb6RMIbrApzl"
   },
   "source": [
    "## Sample Training and Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCJ2W4ZpApzo"
   },
   "outputs": [],
   "source": [
    "# small amount for now so I can test if the code works\n",
    "# train on entire dataset\n",
    "sample_text = all_text[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "colab_type": "code",
    "id": "giR0e15pApzr",
    "outputId": "9881b6b8-3d5d-4e7c-d860-1c4ac795a376"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 247\n",
      "Unique characters: 100\n",
      "Vectorization...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 60, 32)            12768     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 64)                18624     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               6500      \n",
      "=================================================================\n",
      "Total params: 37,892\n",
      "Trainable params: 37,892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/15\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 4.5204\n",
      "Epoch 2/15\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 4.0150\n",
      "Epoch 3/15\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 3.6995\n",
      "Epoch 4/15\n",
      "247/247 [==============================] - 0s 994us/step - loss: 3.5221\n",
      "Epoch 5/15\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 3.4574\n",
      "Epoch 6/15\n",
      "247/247 [==============================] - 0s 992us/step - loss: 3.4493\n",
      "Epoch 7/15\n",
      "247/247 [==============================] - 0s 992us/step - loss: 3.4696\n",
      "Epoch 8/15\n",
      "247/247 [==============================] - 0s 989us/step - loss: 3.4884\n",
      "Epoch 9/15\n",
      "247/247 [==============================] - 0s 963us/step - loss: 3.4724\n",
      "Epoch 10/15\n",
      "247/247 [==============================] - 0s 979us/step - loss: 3.4846\n",
      "Epoch 11/15\n",
      "247/247 [==============================] - 0s 971us/step - loss: 3.4302\n",
      "Epoch 12/15\n",
      "247/247 [==============================] - 0s 979us/step - loss: 3.4238\n",
      "Epoch 13/15\n",
      "247/247 [==============================] - 0s 976us/step - loss: 3.4665\n",
      "Epoch 14/15\n",
      "247/247 [==============================] - 0s 951us/step - loss: 3.4784\n",
      "Epoch 15/15\n",
      "247/247 [==============================] - 0s 958us/step - loss: 3.4474\n"
     ]
    }
   ],
   "source": [
    "maxlen = 60\n",
    "\n",
    "model, char_indices = train_model_from_text(\n",
    "    sample_text,\n",
    "    maxlen=maxlen,\n",
    "    step=20,\n",
    "    epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "C_B7RkdkApzx",
    "outputId": "a5c6237e-8901-4a64-f38f-ede3e328f43b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating with seed: \"tting_haircut_light_skin_tone:. :tropical_drink:. NYE (inclu\"\n",
      "------ temperature: 0.6\n",
      "tting_haircut_light_skin_tone:. :tropical_drink:. NYE (inclul.:ilem.. ...._ls i.gl cdnrit o.. e..eel:es_m:e.no.ell hi.pnGp enii_s i kiF...eo.i:.uleih.u.. kB.e:.Fl.leliaettegs  i... gwnue.il͡.lle.l eell..s.h.hlei r  s :. i .is ii.hBl_..l_sl_nB.s.i hr....  e .i Wle:laeB:..li deiail.hc_.p_ . li:m. _ ..e ..elf l.l__ese hil e.eot  .hwll  :re._ ͡e.:.ts.:.epe_.F.lkd._ lesmM..oic͡ ͡lhie.mh:i.  d  . aeit  .  .s.o.s. i:. hd:C tl.mcs:  l.n  . dl_.sh.ei: ilu.  iBiitrl. g:.   u.t͡sl l .eBm.em iu :i..iiatie .hi_.- l.d e: .l. .u.p :wmphei .die as .  el .s hd...lee:. pltelit .n  lelelhtl l_hie.w.i.elei ilW..ees:up.lei.eeno i .t. ohe .eu.eei.. I.ed _lel..p.awdd_B.: ..h"
     ]
    }
   ],
   "source": [
    "text_generate(\n",
    "    model, \n",
    "    sample_text, \n",
    "    char_indices, \n",
    "    maxlen=maxlen,\n",
    "    temperature=.6,\n",
    "    textlen=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2PHpswIApz0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Prototype_text_generation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
